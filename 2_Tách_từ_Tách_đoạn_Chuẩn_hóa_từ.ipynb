{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Sentence segmentation - Tách văn bản thành nhiều câu**\n",
        "- Sử dụng thư viện nltk"
      ],
      "metadata": {
        "id": "wudmJSZOAC5j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRQeSgdm9zYP",
        "outputId": "62515a98-020c-48a9-882e-69381464a79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello!', 'My name is Hao.', 'What your name ?']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello! My name is Hao. What your name ?\"\n",
        "sent_tokenize(text)\n",
        "print(sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Segmenting word - Tách từ\n"
      ],
      "metadata": {
        "id": "Pi7bpYFaBdbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Xin chào, tôi là Hào.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C768JgBB6k1",
        "outputId": "b282cf1d-ae2d-4815-a7cf-7fd1495ff007"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Xin', 'chào', ',', 'tôi', 'là', 'Hào', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Chuẩn hóa từ - Text Normalization**\n",
        "- ***Các bước chính để chuẩn hóa từ***\n",
        "+ Chuyển về chữ thường\n",
        "+ Loại bỏ dấu cấu\n",
        "+ Loại bỏ khoảng trắng thừa\n",
        "+ Chuyển đổi số thành chữ\n",
        "+ Loại bỏ stopword\n",
        "+ Đưa các từ đồng nghĩa về chung một dạng\n",
        "+ Lemmatization (Chuyển từ về dạng gốc)"
      ],
      "metadata": {
        "id": "ka4unXLNCFFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns3M0Cx-F3DU",
        "outputId": "254460fe-740e-4d15-e233-0a191d806001"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.6.1)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: sklearn-crfsuite, pyvi\n",
            "Successfully installed pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from underthesea import word_tokenize\n",
        "\n",
        "# Tự định nghĩa tokenizer cho tiếng Việt dùng underthesea trong spaCy\n",
        "class VietnameseTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = word_tokenize(text)\n",
        "        spaces = [True] * (len(words) - 1) + [False]\n",
        "        return spacy.tokens.Doc(self.vocab, words=words, spaces=spaces)\n",
        "\n",
        "# Từ điển lemmatization cơ bản\n",
        "lemmatization_dict = {\n",
        "    'học tập': 'học',\n",
        "    'học hành': 'học',\n",
        "    'học hỏi': 'học',\n",
        "    'nghiên cứu': 'nghiên cứu',\n",
        "    'làm việc': 'làm',\n",
        "    'giúp đỡ': 'giúp',\n",
        "    'phát triển': 'phát triển',\n",
        "    'ứng dụng': 'ứng dụng',\n",
        "    'gặp gỡ': 'gặp',\n",
        "}\n",
        "\n",
        "# Hàm lemmatize tùy chỉnh\n",
        "def lemmatize_word(word):\n",
        "    return lemmatization_dict.get(word, word)\n",
        "\n",
        "# Tạo nlp spaCy trống cho tiếng Việt\n",
        "nlp = spacy.blank(\"vi\")\n",
        "nlp.tokenizer = VietnameseTokenizer(nlp.vocab)\n",
        "\n",
        "# Hàm tiền xử lý và chuẩn hóa văn bản\n",
        "def text_normalize(text):\n",
        "    # 1. Chuyển về chữ thường\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Loại bỏ dấu câu và ký tự đặc biệt (ngoại trừ chữ cái và khoảng trắng)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 3. Thay thế số thành 'NUM'\n",
        "    text = re.sub(r'\\d+', 'NUM', text)\n",
        "\n",
        "    # 4. Chuẩn hóa khoảng trắng\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 5. Phân tích và tokenize bằng spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # 6. Lemmatization theo từ điển\n",
        "    lemmatized_words = [lemmatize_word(token.text) for token in doc]\n",
        "\n",
        "    # 7. Loại bỏ stop words cơ bản\n",
        "    stop_words = {'là', 'của', 'một', 'những', 'các', 'và', 'với', 'cho', 'đã', 'trên',\n",
        "                  'này', 'bị', 'được', 'do', 'vào', 'ra', 'tại', 'như', 'thì', 'đang', 'sẽ',\n",
        "                  'đến', 'từ', 'qua', 'khi', 'để', 'về', 'nên', 'cũng', 'theo', 'làm', 'có',\n",
        "                  'đi', 'rất', 'nhưng', 'lại', 'hay', 'vẫn', 'vậy', 'nữa'}\n",
        "    filtered_words = [word for word in lemmatized_words if word not in stop_words]\n",
        "\n",
        "    # 8. Kết quả cuối cùng\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "# Đoạn văn bản mẫu\n",
        "text = \"\"\"\n",
        "Hôm nay, tôi đã đi đến trường Đại học Bách Khoa để tham gia một buổi hội thảo về trí tuệ nhân tạo.\n",
        "Buổi thảo luận rất thú vị và giúp tôi hiểu thêm về các ứng dụng của học máy và xử lý ngôn ngữ tự nhiên.\n",
        "Ngoài ra, tôi cũng gặp gỡ những bạn có cùng đam mê với mình.\n",
        "\"\"\"\n",
        "\n",
        "normalized_text = text_normalize(text)\n",
        "print(normalized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCIMvy2VCl3V",
        "outputId": "24517173-d82e-4019-dcf5-4506ea8713b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hôm nay tôi trường đại học bách khoa tham gia buổi hội thảo trí tuệ nhân tạo buổi thảo luận thú vị giúp tôi hiểu thêm ứng dụng học máy xử lý ngôn ngữ tự nhiên ngoài ra tôi gặp bạn cùng đam mê mình\n"
          ]
        }
      ]
    }
  ]
}